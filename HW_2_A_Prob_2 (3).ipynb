{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PROBLEM 2 : KMeans on data\n",
        "Using Euclidian distance or dot product similarity (choose one per dataset, you can try other similarity metrics). <br>\n",
        "A) run KMeans on the MNIST Dataset, try K=10 <br>\n",
        "B) run KMeans on the FASHION Dataset, try K=10 <br>\n",
        "C) run KMeans on the 20NG Dataset, try K=20 <br>\n",
        "You can use a library for distance/similarity but you have to implement your own kmeans (EM steps, termination criteria etc). <br>\n",
        "For all three datasets, evaluate the KMeans objective for a higher K (for example double) or smaller K(for example half). <br>\n",
        "For all three datasets, evaluate external clustering performance using data labels and performance metrics Purity and Gini Index (see [A] book section 6.9.2)."
      ],
      "metadata": {
        "id": "tKctbiCXm7uO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Kmeans.py\n",
        "import numpy as np\n",
        "\n",
        "class KMeans:\n",
        "    def __init__(self,k,dist_type,iters,num_of_true_lbls):\n",
        "        self.k = k # number of clusters\n",
        "        self.dist_type = dist_type\n",
        "        self.iters = iters\n",
        "        self.num_of_true_lbls = num_of_true_lbls\n",
        "        self.pi = None\n",
        "        self.data = None\n",
        "        self.centroids = None\n",
        "        self.true_lbls = None\n",
        "\n",
        "    def distance(self,x,y):\n",
        "        if(self.dist_type == 'Euclidean'):\n",
        "            return np.linalg.norm(x-y)\n",
        "        elif(self.dist_type == 'Cosine Similarity'):\n",
        "            return np.dot(x,y) / (np.linalg.norm(x)*np.linalg.norm(y))\n",
        "\n",
        "    def fit(self,data,true_lbls):\n",
        "        self.data = data\n",
        "        self.true_lbls = true_lbls\n",
        "        # initializing centroids\n",
        "        indices = np.random.choice(data.shape[0], self.k, replace=False)\n",
        "        self.centroids = data[indices]\n",
        "\n",
        "    def computePi(self):\n",
        "        # for all data points find the closest centroid and update pi\n",
        "        # reinitializing pi everytime it gets recomputed\n",
        "        self.pi = np.zeros((len(self.data),self.k), dtype=int)\n",
        "\n",
        "        for i in range(len(self.data)):\n",
        "            dist = self.distance(self.data[i],self.centroids[0])\n",
        "            closest_centroid_idx = 0\n",
        "            for centroid_idx in range(1,len(self.centroids)):\n",
        "                if(self.distance(self.data[i],self.centroids[centroid_idx]) < dist):\n",
        "                    dist = self.distance(self.data[i],self.centroids[centroid_idx])\n",
        "                    closest_centroid_idx = centroid_idx\n",
        "\n",
        "            self.pi[i][closest_centroid_idx] = 1\n",
        "\n",
        "    def computeCentroids(self):\n",
        "        # for all k clusters\n",
        "        # pi[i] (reshaped to 1xN) is multiplied with Xi (NxD)\n",
        "        # normalized by num of data points in cluster k i.e. sum(pi[i])\n",
        "        for k in range(self.k):\n",
        "            self.centroids[k] = self.pi.T[k] @ self.data / sum(self.pi.T[k])\n",
        "\n",
        "    def predict(self):\n",
        "        # returns cluster lbl allocated to each data point\n",
        "        iters = 0\n",
        "        self.computePi()\n",
        "        self.computeCentroids()\n",
        "        old_objective_value = float('inf')\n",
        "        new_objective_value = self.kmeansObjective()\n",
        "\n",
        "        while iters < self.iters and abs(old_objective_value - new_objective_value) > 1e-6:\n",
        "            self.computePi()\n",
        "            if iters != self.iters - 1:\n",
        "                self.computeCentroids()\n",
        "\n",
        "            old_objective_value = new_objective_value\n",
        "            new_objective_value = self.kmeansObjective()\n",
        "            iters += 1\n",
        "\n",
        "        print(f'Objective function value: {new_objective_value}')\n",
        "        return np.argmax(self.pi, axis=1)\n",
        "\n",
        "\n",
        "    def kmeansObjective(self):\n",
        "        distances_squared = np.sum((self.data[:, np.newaxis] - self.centroids) ** 2, axis=2) # NxK matrix: Dist of each pt with each centroid\n",
        "        filtered_distances = distances_squared * self.pi # Element wise multiplication of distances_sq with membership matrix\n",
        "        return np.sum(filtered_distances) # Sum of all filtered distances\n",
        "\n",
        "    def evaluteClustering(self):\n",
        "        # Need to make confusion matrix of algorithm determined cluster indices (row) vs true cluster indices (column)\n",
        "        # purity = sum of row wise max / total data points\n",
        "        # Gini index for a row (algorithm determined cluster) [Gj] = 1- sum of(mij/Mj)^2 [i from 1 to number of true cluster]\n",
        "        # Gini average = Gj * Mj/ total data points\n",
        "\n",
        "        # creating confusion matrix\n",
        "        algo_det_lbls = self.predict() # array of shape 1xN\n",
        "        cm = np.zeros((self.k,self.num_of_true_lbls), dtype=int)\n",
        "\n",
        "        for i in range(len(algo_det_lbls)):\n",
        "            algo_det_lbl = algo_det_lbls[i]\n",
        "            true_lbl = self.true_lbls[i]\n",
        "            cm[algo_det_lbl][true_lbl] += 1\n",
        "\n",
        "        Pj_sum = np.sum(np.max(cm,axis=1))\n",
        "        print(f\"Purity: {Pj_sum/len(algo_det_lbls)}\")\n",
        "\n",
        "        Gj = []\n",
        "        Mj = np.sum(cm,axis=1) # number of data points per cluster\n",
        "        for i in range(len(cm)):\n",
        "            mij = np.sum(cm[i] ** 2)\n",
        "            if(Mj[i] == 0):\n",
        "                Gj.append(0)\n",
        "            else:\n",
        "                Gj.append(1-(mij/Mj[i]**2))\n",
        "\n",
        "        gini_avg = np.sum(Gj*Mj)/len(algo_det_lbls)\n",
        "        print(f\"Gini Average: {gini_avg}\")"
      ],
      "metadata": {
        "id": "dkddF3fRm2lN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# main.py\n",
        "import numpy as np\n",
        "import idx2numpy\n",
        "import matplotlib.pyplot as plt\n",
        "from KMeans import KMeans\n",
        "import pickle\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "def transform_mnist(data):\n",
        "       transformed_data = data.reshape(data.shape[0],-1)\n",
        "       transformed_data[transformed_data>0] = 1\n",
        "       return transformed_data\n",
        "\n",
        "def transform_fashion(data):\n",
        "       transformed_data = data.reshape(data.shape[0],-1)\n",
        "       return transformed_data\n",
        "\n",
        "def transform_news_groups(data):\n",
        "        pass\n",
        "\n",
        "def visualizeImg(img,lbl,reshaped=False):\n",
        "        i = 3\n",
        "        if(reshaped):\n",
        "               plt.imshow(img.reshape((28,28)), cmap='viridis', interpolation='nearest')\n",
        "        else:\n",
        "               plt.imshow(img, cmap='viridis', interpolation='nearest')\n",
        "        plt.colorbar()  # Show color scale\n",
        "        plt.title(f'Image of {lbl}')\n",
        "        plt.xlabel('X-axis')\n",
        "        plt.ylabel('Y-axis')\n",
        "        plt.show()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "       #imgs = idx2numpy.convert_from_file(\"Datasets/Fashion /t10k-images-idx3-ubyte\")\n",
        "       #imgs_copy = np.copy(imgs)\n",
        "       #lbls = idx2numpy.convert_from_file(\"Datasets/Fashion /t10k-labels-idx1-ubyte\")\n",
        "       #lbls_copy = np.copy(lbls)\n",
        "       filename = 'Datasets/20 NG/dataset.pkl'\n",
        "\n",
        "       with open(filename, 'rb') as file:\n",
        "             ng_data = pickle.load(file)\n",
        "\n",
        "       newsgroups_test = fetch_20newsgroups(subset='test',remove=('headers', 'footers', 'quotes'))\n",
        "       lbls = newsgroups_test.target\n",
        "       target_names = newsgroups_test.target_names\n",
        "\n",
        "       text = np.copy(ng_data)\n",
        "       labels = np.copy(lbls)\n",
        "\n",
        "       kmeans = KMeans(k=40,dist_type='Euclidean',iters=10,num_of_true_lbls=20)\n",
        "       kmeans.fit(data=text,true_lbls=labels)\n",
        "       kmeans.evaluteClustering()\n",
        "\n",
        "    #transformed_imgs = transform_mnist(imgs_copy)\n",
        "    #transformed_imgs = transform_fashion(imgs_copy)\n",
        "#     i=121\n",
        "#     visualizeImg(img=imgs_copy[i],lbl=lbls_copy[i])\n",
        "#     visualizeImg(img=transformed_imgs[i],lbl=lbls[i],reshaped=True)\n",
        "\n",
        "    #kmeans = KMeans(k=20,dist_type='Euclidean',iters=25,num_of_true_lbls=10)\n",
        "    #kmeans.fit(data=transformed_imgs,true_lbls=lbls_copy)\n",
        "    #kmeans.evaluteClustering()\n",
        "\n",
        "    # MNIST k=10\n",
        "    #Objective function value: 1485053.0, Purity: 0.2048, Gini Average: 0.8636386695690841\n",
        "\n",
        "    # MNIST k=5\n",
        "    #Objective function value: 1500369.0, Purity: 0.189, Gini Average: 0.8555777734123303\n",
        "\n",
        "    # MNIST k=20\n",
        "    #Objective function value: 1471162.0, Purity: 0.1993, Gini Average: 0.8580059860829399\n",
        "\n",
        "    # FASHION k=10\n",
        "    #Objective function value: 489841403.0, Purity: 0.1618, Gini Average: 0.8811556562125022\n",
        "\n",
        "    # FASHION k=5\n",
        "    #Objective function value: 770137032.0, Purity: 0.2008, Gini Average: 0.8542479519654073\n",
        "\n",
        "    # FASHION k=20\n",
        "    #Objective function value: 477813985.0, Purity: 0.1627, Gini Average: 0.8795803409135767\n",
        "\n",
        "    # 20NG k=20\n",
        "    # Objective function value: 222293.24200732622, Purity: 0.08470525756771109, Gini Average: 0.9328196371857366\n",
        "\n",
        "    # 20NG k=10\n",
        "    # Objective function value: 248522.20982834254, Purity: 0.07514604354753053, Gini Average: 0.9416480514077292\n",
        "\n",
        "    # 20NG k=40\n",
        "    # RuntimeWarning: invalid value encountered in divide\n",
        "    # self.centroids[k] = self.pi.T[k] @ self.data / sum(self.pi.T[k])\n",
        "    # Objective function value: nan, Purity: 0.09240573552841211, Gini Average: 0.930551486020236\n",
        "    # probably one of the cluster didn't get any points near it. Hence the corresponding col in membership mat is zero\n",
        "\n",
        "#     centroids = kmeans.centroids\n",
        "#     for i in range(len(centroids)):\n",
        "#            visualizeImg(centroids[i],0,reshaped=True)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "wUfVh7WBm_ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#How I preprocessed 20 NG"
      ],
      "metadata": {
        "id": "HYD2crAlnCnx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1C4OfPkUL2Rw",
        "outputId": "60813f4a-72f7-4489-a8f1-dc5ae39eeab4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from collections import Counter\n",
        "import math\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from sklearn.datasets import load_files\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "nltk.download('wordnet')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = '/content/drive/MyDrive/USML HW2/newsgrps_test.pkl'\n",
        "url2 = '/content/drive/MyDrive/USML HW2/newsgrps_test_no_footers_headers_quotes.pkl'"
      ],
      "metadata": {
        "id": "lfi3creeMESk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(url2, 'rb') as file:\n",
        "  ng_test = pickle.load(file)"
      ],
      "metadata": {
        "id": "HNcMqo5xMPp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(ng_test.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2IuidgTOjJC",
        "outputId": "b0a09b00-4402-41d1-dadb-3437d197a1eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7532"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(ng_test.target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deF1lR41OlrH",
        "outputId": "1df08d01-3db3-4fc7-87ea-0c9c65ebe105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7532"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 10\n",
        "print(ng_test.data[i],ng_test.target_names[ng_test.target[i]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiBvUZXKOqkd",
        "outputId": "1c6c37d7-110c-4917-d2ca-dba56d853e56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From: Greg.Reinacker@FtCollins.NCR.COM\n",
            "Subject: Windows On-Line Review uploaded\n",
            "Reply-To: Greg.Reinacker@FtCollinsCO.NCR.COM\n",
            "Organization: NCR Microelectronics, Ft. Collins, CO\n",
            "Lines: 12\n",
            "\n",
            "I have uploaded the Windows On-Line Review shareware edition to\n",
            "ftp.cica.indiana.edu as /pub/pc/win3/uploads/wolrs7.zip.\n",
            "\n",
            "It is an on-line magazine which contains reviews of some shareware\n",
            "products...I grabbed it from the Windows On-Line BBS.\n",
            "\n",
            "--\n",
            "--------------------------------------------------------------------------\n",
            "Greg Reinacker                          (303) 223-5100 x9289\n",
            "NCR Microelectronic Products Division   VoicePlus 464-9289\n",
            "2001 Danfield Court                     Greg.Reinacker@FtCollinsCO.NCR.COM\n",
            "Fort Collins, CO  80525\n",
            " comp.os.ms-windows.misc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "print(ng_test.data[i])\n",
        "print(ng_test.target_names[ng_test.target[i]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ98YMHaOwJX",
        "outputId": "f74ea8c0-02b1-4377-9056-58dd63ed6a70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am a little confused on all of the models of the 88-89 bonnevilles.\n",
            "I have heard of the LE SE LSE SSE SSEI. Could someone tell me the\n",
            "differences are far as features or performance. I am also curious to\n",
            "know what the book value is for prefereably the 89 model. And how much\n",
            "less than book value can you usually get them for. In other words how\n",
            "much are they in demand this time of year. I have heard that the mid-spring\n",
            "early summer is the best time to buy.\n",
            "rec.autos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_text = ''\n",
        "for i in range(len(ng_test.data)):\n",
        "  total_text += ng_test.data[i]"
      ],
      "metadata": {
        "id": "ekG_YNXQkyTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = ng_test.data[0]"
      ],
      "metadata": {
        "id": "cI7FyIg4mFYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(total_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8A_mtpSmISy",
        "outputId": "d1339367-e41f-4e7c-ebcc-f0736a11b45c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8261569"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "def is_valid_word(word):\n",
        "  return bool(wordnet.synsets(word))\n",
        "\n",
        "total_text = re.sub(r'[^a-zA-Z0-9 \\n]', '', total_text)\n",
        "total_text = re.sub(r'\\n+', ' ', total_text)\n",
        "total_text = total_text.lower()\n",
        "tokens = word_tokenize(total_text)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = [word for word in tokens if word not in stop_words]\n",
        "filtered_tokens = [word for word in tokens if is_valid_word(word)]"
      ],
      "metadata": {
        "id": "Vuaw_rIBlpV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0iGQVyMSlvK",
        "outputId": "d3126558-d0b7-42d5-b06f-d7f21d41bf52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "601285"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_tokens = set(filtered_tokens)"
      ],
      "metadata": {
        "id": "QCJE5LgKn6xL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oY9mzBWVoC_K",
        "outputId": "e53b151f-cbcb-4c3b-cc3f-58db77395156"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28892"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_tokens_5k = random.sample(filtered_tokens, 5000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_PTXgA1qk5R",
        "outputId": "0ea6ac47-9c71-4176-9e0c-d1560c0a7e2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-70-ffbac7204284>:1: DeprecationWarning: Sampling from a set deprecated\n",
            "since Python 3.9 and will be removed in a subsequent version.\n",
            "  filtered_tokens_5k = random.sample(filtered_tokens, 5000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_tokens_5k.sort()"
      ],
      "metadata": {
        "id": "sjhTKfpXq0p1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "  text = re.sub(r'[^a-zA-Z0-9 \\n]', '', text)\n",
        "  text = re.sub(r'\\n+', ' ', text)\n",
        "  text = text.lower()\n",
        "  tokens = word_tokenize(text)\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [word for word in tokens if word not in stop_words]\n",
        "  dict_tokens = dict(Counter(tokens))\n",
        "\n",
        "  np_arr = np.zeros(5000)\n",
        "  for i in range(len(filtered_tokens_5k)):\n",
        "    if filtered_tokens_5k[i] in dict_tokens:\n",
        "      np_arr[i] = dict_tokens[filtered_tokens_5k[i]]\n",
        "  return np_arr"
      ],
      "metadata": {
        "id": "YKwKhayrrJae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np_dataset = []\n",
        "\n",
        "for i in range(len(ng_test.data)):\n",
        "  np_dataset.append(preprocess_text(ng_test.data[i]))"
      ],
      "metadata": {
        "id": "Y_JyB_OToLud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 0\n",
        "for i in range(len(np_dataset[idx])):\n",
        "  if np_dataset[idx][i] != 0:\n",
        "    print(filtered_tokens_5k[i],np_dataset[idx][i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkdcjPbpsJaC",
        "outputId": "52ff7854-33f5-4705-b74c-549f0b3a1a44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "book 2.0\n",
            "heard 2.0\n",
            "le 1.0\n",
            "much 2.0\n",
            "usually 1.0\n",
            "words 1.0\n",
            "year 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = np.array(np_dataset)"
      ],
      "metadata": {
        "id": "Y52o7PdCssVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFyrC_fAuV16",
        "outputId": "c7f52098-841a-4ea2-c7fe-7d8734807e20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7532, 5000)"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.nbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KsxmcHaunTR",
        "outputId": "25de0f28-929d-4c76-848c-5b3af9d144d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "301280000"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zero_count = np.count_nonzero(dataset == 0)\n",
        "\n",
        "# Calculate the total number of elements in the matrix\n",
        "total_elements = dataset.size\n",
        "\n",
        "# Calculate the sparsity (percentage of zero elements)\n",
        "sparsity = (zero_count / total_elements) * 100\n",
        "\n",
        "print(f\"The sparsity of the dataset is {sparsity:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xjea14b4uq2R",
        "outputId": "b748a24a-40ff-40a7-f217-53e588e45893"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sparsity of the dataset is 99.82%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = '/content/drive/MyDrive/USML HW2/dataset.pkl'\n",
        "\n",
        "with open(filename, 'wb') as file:\n",
        "  pickle.dump(dataset, file)\n"
      ],
      "metadata": {
        "id": "eNfOWctJu9R0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 0\n",
        "for i in range(len(dataset[idx])):\n",
        "  if dataset[idx][i] != 0:\n",
        "    print(filtered_tokens_5k[i],dataset[idx][i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFBmSNE_xNVo",
        "outputId": "3fd1ccb6-46ba-4146-9b6a-75d3149040f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "book 2.0\n",
            "heard 2.0\n",
            "le 1.0\n",
            "much 2.0\n",
            "usually 1.0\n",
            "words 1.0\n",
            "year 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ng_test.data[0])\n",
        "print(ng_test.target_names[ng_test.target[0]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9GlKIoByLZx",
        "outputId": "f7fa8390-fd61-48ac-bc59-c58f034b3fc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am a little confused on all of the models of the 88-89 bonnevilles.\n",
            "I have heard of the LE SE LSE SSE SSEI. Could someone tell me the\n",
            "differences are far as features or performance. I am also curious to\n",
            "know what the book value is for prefereably the 89 model. And how much\n",
            "less than book value can you usually get them for. In other words how\n",
            "much are they in demand this time of year. I have heard that the mid-spring\n",
            "early summer is the best time to buy.\n",
            "rec.autos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2lQg7ZZPyW1J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}